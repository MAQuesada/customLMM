{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LlamaIndex: the ultimate LLM framework for indexing and retrieval**[<sup>source<sup>](https://towardsdatascience.com/llamaindex-the-ultimate-llm-framework-for-indexing-and-retrieval-fa588d8ca03e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`LlamaIndex`** , previously known as the **GPT Index**, is a remarkable data framework aimed at helping you build applications with LLMs by providing essential tools that facilitate data ingestion, structuring, retrieval, and integration with various application frameworks. The capabilities offered by LlamaIndex are numerous and highly valuable:\n",
    "\n",
    "✅ Ingest from different data sources and data formats using `Data connectors` (Llama Hub).\n",
    "\n",
    "✅ Enable document operations such as inserting, deleting, updating, and refreshing the document index.\n",
    "\n",
    "✅ Support synthesis over heterogeneous data and multiple documents.\n",
    "\n",
    "✅ Use “Router” to pick between different query engines.\n",
    "\n",
    "✅ Allow for the hypothetical document embeddings to enhance output quality\n",
    "\n",
    "✅ Offer a wide range of integrations with various vector stores, ChatGPT plugins, tracing tools, and LangChain, among others.\n",
    "\n",
    "✅ Support the brand new OpenAI function calling API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data connectors (LlamaHub)\n",
    "The [Llama Hub](https://llamahub.ai) offers a wide range of over 100 data sources and formats, allowing LlamaIndex or LangChain to ingest data in a consistent manner. \n",
    " \n",
    "By default, you can ``` pip install llama-hub```  and use it as a standalone package. You may also choose to use our `download_loader` method to individually download a data loader for use with LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_hub.wikipedia.base import WikipediaReader\n",
    "from llama_index import download_loader\n",
    "\n",
    "# another ways\n",
    "# WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "loader = WikipediaReader()\n",
    "documents = loader.load_data(pages=['Berlin', 'Rome', 'Tokyo', 'Canberra', 'Santiago'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada documento contiene: \n",
    "```python \n",
    "'id_', 'embedding', 'metadata', 'excluded_embed_metadata_keys', 'excluded_llm_metadata_keys', 'relationships', 'hash', 'text', 'start_char_idx', 'end_char_idx', 'text_template', 'metadata_template', 'metadata_seperator'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "import os\n",
    "from  dotenv import load_dotenv\n",
    "\n",
    "# load the variables located in the file .env\n",
    "load_dotenv()\n",
    "# print(os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  F:\\\\DOCUMENTOS\\\\DATA_SCIENCE\\\\Large Language Models LLM\\\\ggml-gpt4all-j-v1.3-groovy.bin\n",
      "Found model file at  F:\\\\DOCUMENTOS\\\\DATA_SCIENCE\\\\Large Language Models LLM\\\\ggml-gpt4all-j-v1.3-groovy.bin\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "local_path = 'F:\\\\DOCUMENTOS\\\\DATA_SCIENCE\\Large Language Models LLM\\\\ggml-gpt4all-j-v1.3-groovy.bin'\n",
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "# Verbose is required to pass to the callback manager\n",
    "llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True, backend='gptj')\n",
    "llm_local = GPT4All(model=local_path, callbacks=callbacks, verbose=True, backend='gptj')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import LangChainLLM\n",
    "from llama_index.indices.service_context import ServiceContext\n",
    "\n",
    "llm = LangChainLLM(llm_local)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model='local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic query functionalities\n",
    "**Index, retriever, and query** engine are three basic components for asking questions over your data or documents:\n",
    "\n",
    "* `Index` is a data structure that allows us to retrieve relevant information quickly for a user query from external documents. Index works by parsing documents into text chunks, which are called ***“Node”*** objects, and then building index from the chunks.\n",
    "\n",
    "* `Retriever` is used for fetching and retrieving relevant information given user query.\n",
    "\n",
    "* `Query engine` is built on top of index and retriever providing a generic interface to ask questions about your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham was a prominent Australian businessman who made his fortune in retailing during the 1950s and 1960s through his company \"The Piggly Wigglys\". He later became involved with various philanthropic organizations such as the National Museum of Australia Foundation and the Canberra Symphony Orchestra, where he served on their board for many years. Graham was also a strong supporter of education and established several schools in Canberra including St John's School (now known as The Australian College) which is now part of the University of Notre Dame Australia. He passed away at his home in Red Hill in 2019 after suffering from Alzheimer's disease, leaving behind an extensive legacy that continues to be celebrated by many people today."
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Who is Paul Graham.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The population of Berlin as of 2019 was approximately 5.2 million people within a city area of 891.1 km2 (344.1 sq mi)."
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the population in Berlin actually ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle document updates\n",
    "Often times, once we create an index for our document, there might be a need to periodically update the document. This process can be costly if we were to recreate the embeddings for the entire document again. LlamaIndex index structure offers a solution by enabling efficient insertion, deletion, update, and refresh operations.\n",
    "\n",
    "For example, a new document can be inserted as additional nodes (text chunks) without the need to recreate nodes from previous documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://gpt-index.readthedocs.io/en/latest/how_to/index/document_management.html\n",
    "from llama_index import ListIndex, Document\n",
    "\n",
    "index = ListIndex([], service_context=service_context)\n",
    "text_chunks = ['text_chunk_1', 'text_chunk_2', 'text_chunk_3']\n",
    "\n",
    "doc_chunks = []\n",
    "for i, text in enumerate(text_chunks):\n",
    "    doc = Document(text=text, doc_id=f\"doc_id_{i}\")\n",
    "    doc_chunks.append(doc)\n",
    "\n",
    "# insert\n",
    "for doc_chunk in doc_chunks:\n",
    "    index.insert(doc_chunk,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query multiple documents\n",
    "With LlamaIndex, it’s easy to query multiple documents. This functionality is enabled through the `SubQuestionQueryEngine` class. When given a query, the query engine generates a “query plan” consisting of sub-queries against sub-documents, which are then synthesized to provide the final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "    {\n",
      "        \"sub_question\": \"What is your name\",\n",
      "        \"tool_name\": \"jo_brow\"\n",
      "    },\n",
      "    {\n",
      "        \"sub_question\": \"Who are the candidates you have?\",\n",
      "        \"tool_name\": \"david_smith\"\n",
      "    }\n",
      "]\n",
      "```Generated 2 sub questions.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mf:\\DOCUMENTOS\\DATA_SCIENCE\\Large Language Models LLM\\custome_LMM\\starting_with_llamaIndex.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/DOCUMENTOS/DATA_SCIENCE/Large%20Language%20Models%20LLM/custome_LMM/starting_with_llamaIndex.ipynb#X20sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m question_gen \u001b[39m=\u001b[39m LLMQuestionGenerator\u001b[39m.\u001b[39mfrom_defaults(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/DOCUMENTOS/DATA_SCIENCE/Large%20Language%20Models%20LLM/custome_LMM/starting_with_llamaIndex.ipynb#X20sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m                         service_context\u001b[39m=\u001b[39mservice_context\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/DOCUMENTOS/DATA_SCIENCE/Large%20Language%20Models%20LLM/custome_LMM/starting_with_llamaIndex.ipynb#X20sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m                     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/DOCUMENTOS/DATA_SCIENCE/Large%20Language%20Models%20LLM/custome_LMM/starting_with_llamaIndex.ipynb#X20sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m s_engine \u001b[39m=\u001b[39m SubQuestionQueryEngine\u001b[39m.\u001b[39mfrom_defaults(query_engine_tools\u001b[39m=\u001b[39mquery_engine_tools,service_context\u001b[39m=\u001b[39mservice_context,question_gen\u001b[39m=\u001b[39mquestion_gen)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/DOCUMENTOS/DATA_SCIENCE/Large%20Language%20Models%20LLM/custome_LMM/starting_with_llamaIndex.ipynb#X20sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m response \u001b[39m=\u001b[39m s_engine\u001b[39m.\u001b[39;49mquery(\u001b[39m'\u001b[39;49m\u001b[39mGive me the names of the candidates you have\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Manue!_PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\llama_index\\indices\\query\\base.py:23\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(str_or_query_bundle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m     22\u001b[0m     str_or_query_bundle \u001b[39m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[1;32m---> 23\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(str_or_query_bundle)\n\u001b[0;32m     24\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\Manue!_PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\llama_index\\query_engine\\sub_question_query_engine.py:139\u001b[0m, in \u001b[0;36mSubQuestionQueryEngine._query\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_async:\n\u001b[0;32m    134\u001b[0m     tasks \u001b[39m=\u001b[39m [\n\u001b[0;32m    135\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aquery_subq(sub_q, color\u001b[39m=\u001b[39mcolors[\u001b[39mstr\u001b[39m(ind)])\n\u001b[0;32m    136\u001b[0m         \u001b[39mfor\u001b[39;00m ind, sub_q \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sub_questions)\n\u001b[0;32m    137\u001b[0m     ]\n\u001b[1;32m--> 139\u001b[0m     qa_pairs_all \u001b[39m=\u001b[39m run_async_tasks(tasks)\n\u001b[0;32m    140\u001b[0m     qa_pairs_all \u001b[39m=\u001b[39m cast(List[Optional[SubQuestionAnswerPair]], qa_pairs_all)\n\u001b[0;32m    141\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Manue!_PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\llama_index\\async_utils.py:39\u001b[0m, in \u001b[0;36mrun_async_tasks\u001b[1;34m(tasks, show_progress, progress_bar_desc)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_gather\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Any]:\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mgather(\u001b[39m*\u001b[39mtasks_to_execute)\n\u001b[1;32m---> 39\u001b[0m outputs: List[Any] \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39;49mrun(_gather())\n\u001b[0;32m     40\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\Manue!_PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[39mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39mif\u001b[39;00m events\u001b[39m.\u001b[39m_get_running_loop() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m     34\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m coroutines\u001b[39m.\u001b[39miscoroutine(main):\n\u001b[0;32m     37\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39ma coroutine was expected, got \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(main))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "# Source: https://gpt-index.readthedocs.io/en/latest/examples/usecases/10q_sub_question.html\n",
    "\n",
    "from llama_index import download_loader, SimpleDirectoryReader\n",
    "from pathlib import Path\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.response.pprint_utils import pprint_response\n",
    "\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.query_engine import SubQuestionQueryEngine\n",
    "\n",
    "# Load data\n",
    "# PDFReader = download_loader(\"PDFReader\")\n",
    "# loader = PDFReader()\n",
    "document1 = SimpleDirectoryReader(input_files=['resources/cv_david_smith.pdf']).load_data()\n",
    "document2 = SimpleDirectoryReader(input_files=['resources/cv_Jo Brown.pdf']).load_data()\n",
    "\n",
    "\n",
    "# Build indices\n",
    "index1 = VectorStoreIndex.from_documents(document1, service_context=service_context,)\n",
    "index2 = VectorStoreIndex.from_documents(document2, service_context=service_context,)\n",
    "\n",
    "# Build query engines\n",
    "engine1 = index1.as_query_engine(similarity_top_k=3)\n",
    "engine2 = index2.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=engine2,\n",
    "        metadata=ToolMetadata(\n",
    "            name='jo_brow',\n",
    "            description='Provides information about Jo Brow cuurriculum')),\n",
    "    QueryEngineTool(\n",
    "        query_engine=engine1,\n",
    "        metadata=ToolMetadata(\n",
    "            name='david_smith',\n",
    "            description='Provides information about David Smith cuurriculum')), ]\n",
    "\n",
    "# Run \n",
    "from llama_index.question_gen.llm_generators import LLMQuestionGenerator\n",
    "question_gen = LLMQuestionGenerator.from_defaults(\n",
    "                        service_context=service_context\n",
    "                    )\n",
    "s_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools,service_context=service_context,question_gen=question_gen)\n",
    "response = s_engine.query('Give me the names of the candidates you have')\n",
    "\n",
    "## da RunTime Error al general dos subquestion puede ser producto de que estoy forzaando a usar un LLM Local y un embeddings open source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use “Router” to pick between different query engines\n",
    "\n",
    "Imagine you are building a bot to retrieve information from both Notion and Slack, how does the language model know which tool to use to search for information? LlamaIndex is like a clever helper that can find things for you, even if they are in different places. Specifically, LlamaIndex’s “Router” is a super simple abstraction that allows “picking” between different query engines.\n",
    "\n",
    "In this example, we have two document indexes from Notion and Slack, and we create two query engines for each of them. After that, we put all the tools together and create a super tool called RouterQueryEngine, which picks which tool to use based on the description we gave to the individual tools. This way, when we ask a question about Notion, the router will automatically look for information from the Notion documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# routing-over-heterogeneous-data\n",
    "from llama_index.query_engine import RouterQueryEngine\n",
    "from llama_index import TreeIndex, VectorStoreIndex\n",
    "from llama_index.tools import QueryEngineTool\n",
    "\n",
    "# define sub-indices\n",
    "index1 = VectorStoreIndex.from_documents(document1, service_context=service_context)\n",
    "index2 = VectorStoreIndex.from_documents(document2, service_context=service_context)\n",
    "\n",
    "# define query engines and tools\n",
    "tool1 = QueryEngineTool.from_defaults(\n",
    "    query_engine=index1.as_query_engine(),\n",
    "    description=\"Use this query engine to obtain data about david smith \",\n",
    ")\n",
    "tool2 = QueryEngineTool.from_defaults(\n",
    "    query_engine=index2.as_query_engine(),\n",
    "    description=\"Use this query engine to obtain data about jo brow\",\n",
    ")\n",
    "query_engine = RouterQueryEngine.from_defaults(\n",
    "    query_engine_tools=[tool1, tool2],service_context=service_context)\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"In Notion, give me a summary of David.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothetical document embeddings (HyDE)\n",
    "Typically, when we ask a question about an external document, what we normally do is that we use text **embeddings** to create *vector representations* for both the *question* and the doc*ument. Then we use *semantic search* to find the text chunks that are the most relevant to the question. However, the answer to the question may differ significantly from the question itself. What if we could **generate hypothetical answers** to our question first and then find the text chunks that are most relevant to the hypothetical answer? That’s where hypothetical document embeddings (HyDE) come into play and can potentially improve output quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After attending RISD for his undergraduate degree in painting, Paul Graham went on to study at Accademia di Belle Arti (ABA) in Florence. He was accepted into their program but had trouble adjusting due to language barriers and cultural differences between the United States and Italy. Despite these challenges, he eventually completed a Master's Degree from ABA with an emphasis in painting. After graduation, Graham returned home to Providence where he worked as a freelance artist for clients such as hedge fund managers and real estate developers. He also continued his studies at RISD by taking classes on color theory and composition."
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>After attending RISD for his undergraduate degree in painting, Paul Graham went on to study at Accademia di Belle Arti (ABA) in Florence. He was accepted into their program but had trouble adjusting due to language barriers and cultural differences between the United States and Italy. Despite these challenges, he eventually completed a Master's Degree from ABA with an emphasis in painting. After graduation, Graham returned home to Providence where he worked as a freelance artist for clients such as hedge fund managers and real estate developers. He also continued his studies at RISD by taking classes on color theory and composition.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Source: https://gpt-index.readthedocs.io/en/latest/examples/query_transformations/HyDEQueryTransformDemo.html\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.query_engine.transform_query_engine import TransformQueryEngine\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader('resources/paul_graham_essay/').load_data()\n",
    "index = VectorStoreIndex.from_documents(documents,service_context=service_context)\n",
    "\n",
    "query_str = \"what did paul graham do after going to RISD\"\n",
    "\n",
    "\n",
    "# First, we query without transformation: The same query string is used for embedding lookup and also summarization.\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(query_str)\n",
    "display(Markdown(f\"<b>{response}</b>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use HyDEQueryTransform to generate a hypothetical document and use it for embedding lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde = HyDEQueryTransform(llm_predictor=llm_local,include_original=True)\n",
    "hyde_query_engine = TransformQueryEngine(query_engine, hyde)\n",
    "response = hyde_query_engine.query(query_str)\n",
    "display(Markdown(f\"<b>{response}</b>\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this example, HyDE improves output quality significantly, by hallucinating accurately what Paul Graham did after RISD (see below), and thus improving the embedding quality, and final output.\n",
    "query_bundle = hyde(query_str)\n",
    "hyde_doc = query_bundle.embedding_strs[0]\n",
    "hyde_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Failure case` HyDE may mislead when query can be mis-interpreted without context. See more in https://github.com/jerryjliu/llama_index/blob/main/docs/examples/query_transformations/HyDEQueryTransformDemo.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LlamaIndex with LangChain\n",
    "\n",
    "`LangChain`, with its extensive list of features, casts a wider net, concentrating on the use of chains and agents to connect with external APIs. On the other hand, `LlamaIndex` has a narrower focus shining in the area of data indexing and document retrieval.\n",
    "\n",
    "Here is an example where we used LlamaIndex to keep the chat history when using a LangChain agent. When we ask “what’s my name?” in the second round of conversation, the language model knows that “I am Bob” from the first round of conversation:\n",
    "https://github.com/jerryjliu/llama_index/blob/main/examples/langchain_demo/LangchainDemo.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using LlamaIndex as a memory module\n",
    "from langchain import OpenAI\n",
    "from langchain.llms import OpenAIChat\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "from llama_index import ListIndex\n",
    "from llama_index.langchain_helpers.memory_wrapper import GPTIndexChatMemory\n",
    "\n",
    "index = ListIndex([])\n",
    "memory = GPTIndexChatMemory(\n",
    "    index=index,\n",
    "    memory_key=\"chat_history\",\n",
    "    query_kwargs={\"response_mode\": \"compact\"},\n",
    "    # return_source returns source nodes instead of querying index\n",
    "    return_source=True,\n",
    "    # return_messages returns context in message format\n",
    "    return_messages=True\n",
    ")\n",
    "llm = OpenAIChat(temperature=0)\n",
    "# llm=OpenAI(temperature=0)\n",
    "agent_executor = initialize_agent([], llm, agent=\"conversational-react-description\", memory=memory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
