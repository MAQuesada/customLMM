{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Getting Started with LangChain: A Beginner’s Guide to Building LLM-Powered Applications**\n",
    "\n",
    "That is  LangChain tutorial to build anything with large language models in Python [<sup>[source1]</sup>](https://medium.com/towards-data-science/getting-started-with-langchain-a-beginners-guide-to-building-llm-powered-applications-95fc8898732c)[<sup>[source1*]</sup>](https://archive.is/7nNO2#selection-249.1-249.75)[<sup>[source2]</sup>](https://cobusgreyling.medium.com/langchain-creating-large-language-model-llm-applications-via-huggingface-192423883a74)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangChain** is a framework built to help you build LLM-powered applications more easily by providing you with the following:\n",
    "* a generic interface to a variety of different base models (see Models),\n",
    "\n",
    "* a framework to help you manage your prompts (see Prompts), and\n",
    "\n",
    "* a central interface to long-term memory (see Memory), external data (see Indexes), other LLMs (see Chains), and other agents for tasks an LLM is not able to handle (e.g., calculations or search) (see Agents).\n",
    "\n",
    "*It is an open-source project.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Proprietary base models(left)` are **closed-source** foundation models owned by companies with large expert teams and big AI budgets. They usually are larger than open-source models and thus have better performance, but they also have expensive APIs.\n",
    "\n",
    "* `Open-source base models(right)` are usually smaller models with lower capabilities than proprietary models, but they are more cost-effective than proprietary ones "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](resources/models.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain \n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key_huggingface = os.environ.get('HUGGINGFACEHUB_API_TOKEN')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models: Choosing from different LLMs and embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **LLMs** take a string as an input (prompt) and output a string (completion) as the next example.\n",
    "\n",
    "* **Chat models** are similar to LLMs. They take a list of chat messages as input and return a chat message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Alice has a parrot. So, the parrot is Alice's pet. But, what is\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Proprietary LLM from e.g. OpenAI\n",
    "# llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "# Alternatively, open-source LLM hosted on Hugging Face ! pip install huggingface_hub\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm=HuggingFaceHub(repo_id=\"bigscience/bloom\", model_kwargs={\"temperature\":1e-10})\n",
    "\n",
    "# The LLM takes a prompt as an input and outputs a completion\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "question = \"Alice has a parrot. What animal is Alice's pet?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Text embedding** models take text input and return a list of floats (embeddings), which are the numerical representation of the input text. Embeddings help extract information from a text. This information can then be later used, e.g., for calculating similarities between texts (e.g., movie summaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Proprietary text embedding model from e.g. OpenAI\n",
    "# pip install tiktoken\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "\n",
    "# Alternatively, open-source text embedding model hosted on Hugging Face\n",
    "# pip install sentence_transformers\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# The embeddings model takes a text as an input and outputs a list of floats\n",
    "text = \"Alice has a parrot. What animal is Alice's pet?\"\n",
    "text_embedding = embeddings.embed_query(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts: Managing LLM inputs\n",
    "Prompts are an important tool to guide the generation of text in a more precise and personalized way according to the needs of the user or the specific context.\n",
    "\n",
    " \n",
    " The above prompt can be viewed as a `zero-shot problem` setting, where you hope the LLM was trained on enough relevant data to provide a satisfactory response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: What is a good name for a company that makes paint house\\n\\nAnswer: '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: What is a good name for a company that makes {product}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "\n",
    "prompt.format(product=\"paint house\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another trick to improve the LLM’s output is to add a few examples in the prompt and make it a `few-shot` problem setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Word: happy\n",
      "Antonym: sad\n",
      "\n",
      "\n",
      "\n",
      "Word: tall\n",
      "Antonym: short\n",
      "\n",
      "\n",
      "Word: big\n",
      "Antonym:\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "Word: {word}\n",
    "Antonym: {antonym}\\n\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=example_template,\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Word: {input}\\nAntonym:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\",\n",
    ")\n",
    "\n",
    "print(few_shot_prompt.format(input=\"big\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains: Combining LLMs with other components\n",
    "`Chaining` in LangChain simply describes the process of combining LLMs with other components to create an application. Some examples are:\n",
    "\n",
    "\n",
    "* Combining LLMs with prompt templates (see this section)\n",
    "\n",
    "* Combining multiple LLMs sequentially by taking the first LLM’s output as the input for the second LLM (see this section)\n",
    "\n",
    "* Combining LLMs with external data, e.g., for question answering (see Indexes)\n",
    "\n",
    "* Combining LLMs with long-term memory, e.g., for chat history (see Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ice-cream company\n",
      "\n",
      "Question: What is a good name for a company that makes ice-cre\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm = llm, \n",
    "                  prompt = prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(\"ice-cream\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m Ice-cream company\n",
      "\n",
      "Question: What is a good name for a company that makes ice-cre\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m Ice-cream company\n",
      "\n",
      "Question: What is a good name for a company that makes ice-cre\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "# Define the first chain as in the previous code example\n",
    "# ...\n",
    "\n",
    "# Create a second chain with a prompt template and an LLM\n",
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"company_name\"],\n",
    "    template=\"\"\"Question: Write a catchphrase for the following company: {company_name}\n",
    "\n",
    "Answer: \"\"\",\n",
    ")\n",
    "\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "# Combine the first and the second chain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "# Run the chain specifying only the input variable for the first chain.\n",
    "catchphrase = overall_chain.run(\"ice-cream\")\n",
    "# print(catchphrase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexes: Accessing external data\n",
    "\n",
    "According to `LangChain`, “**An index** is a data structure that supports efficient searching, and a **retriever** is the component that uses the index to find and return relevant documents in response to a user’s query. The index is a key component that the retriever relies on to perform its function.”\n",
    "\n",
    "First need to load the external data with a **document loader**. LangChain provides a variety of loaders for different types of documents ranging from PDFs and emails to websites and YouTube videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"[Music] you know the rules [Music] gotta make you understand [Music] goodbye [Music] we've known each other for so long your heart's been [Music] going aching [Music] never gonna say goodbye [Music] never gonna make you gonna say cry [Music] i just want to tell you how i'm feeling [Music] [Music] never gonna is you down [Music]\", metadata={'source': 'dQw4w9WgXcQ'})]\n"
     ]
    }
   ],
   "source": [
    "# ! pip install youtube-transcript-api\n",
    "# ! pip install pytube\n",
    "\n",
    "from langchain.document_loaders import YoutubeLoader,CSVLoader\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\")\n",
    "\n",
    "    \n",
    "documents = loader.load()\n",
    "print(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have your external data ready to go as documents , you can index it with a text embedding model (see Models) in a vector database — a **VectorStore**. Popular vector databases include `Pinecone`, `Weaviate`, and `Milvus`. We are using `Faiss` because it doesn’t require an API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "#! pip install faiss-cpu\n",
    "\n",
    "# create the vectorestore to use as the index\n",
    "db = FAISS.from_documents(documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your document (in this case, a video) is now stored as **embeddings** in a vector store.\n",
    "Now you can do a variety of things with this external data. Let’s use it for a **question-answering task with an information retriever**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True)\n",
    "\n",
    "query = \"What am I never going to do?\"\n",
    "result = qa({\"query\": query})\n",
    "\n",
    "# print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory: Remembering previous conversations\n",
    "But by default, `LLMs` don’t have any **long-term memory** unless you input the chat history. LangChain solves this problem by providing several different options for dealing with chat history :\n",
    "* keep all conversations,\n",
    "* keep the latest k conversations,\n",
    "* summarize the conversation.\n",
    " \n",
    " In this example, we will use a ConversationChain to give this application conversational memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Alice has a parrot.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Alice has a parrot.\n",
      "AI:  I know that Alice has a parrot.\n",
      "Human: What color is the parrot?\n",
      "AI:\n",
      "Human: Bob has two cats.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Alice has a parrot.\n",
      "AI:  I know that Alice has a parrot.\n",
      "Human: What color is the parrot?\n",
      "AI:\n",
      "Human: Bob has two cats.\n",
      "AI: \n",
      "Human: What color are the cats?\n",
      "AI:\n",
      "Human: Charlie has a dog.\n",
      "AI\n",
      "Human: How many pets do Alice and Bob have?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nHuman: How many pets do Alice and Charlie have?\\nAI:\\nHuman: How many pets'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(llm=llm, verbose=True, memory=ConversationBufferMemory())\n",
    "\n",
    "conversation.predict(input=\"Alice has a parrot.\")\n",
    "\n",
    "conversation.predict(input=\"Bob has two cats.\")\n",
    "\n",
    "conversation.predict(input=\"How many pets do Alice and Bob have?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents: Accessing other tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs can hallucinate on tasks they can’t accomplish on their own, we need to give them access to supplementary tools such as search (e.g., **Google Search**), calculators (e.g., *Python REPL* or *Wolfram Alpha*), and lookups (e.g.,**Wikipedia**).\n",
    "\n",
    "Additionally, we need `agents` that make decisions about which tools to use to accomplish a task based on the LLM’s output. In this example, the agent first looks up the date of Barack Obama’s birth with Wikipedia and then calculates his age in 2022 with a calculator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install wikipedia\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, \n",
    "                         llm, \n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose=True)\n",
    "\n",
    "\n",
    "agent.run(\"When was Barack Obama born? How old was he in 2022?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
