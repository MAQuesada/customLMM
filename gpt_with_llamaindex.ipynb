{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Connecting ChatGPT with Your Own Data using LlamaIndex [<sup>source<sup>](https://levelup.gitconnected.com/connecting-chatgpt-with-your-own-data-using-llamaindex-663844c06653)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requeriments\n",
    "# project that provides a central interface to connect your LLM’s with external data\n",
    "! pip install llama_index\n",
    "\n",
    "# framework for developing applications powered by language models\n",
    "! pip install langchain\n",
    "\n",
    "\n",
    "! pip install pypdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the `OPENAI_API_KEY ` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from  dotenv import load_dotenv\n",
    "\n",
    "# load the variables located in the file .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, GPTVectorStoreIndex,\\\n",
    "    LLMPredictor, PromptHelper\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI \n",
    "\n",
    "\n",
    "# GPTVectorStoreIndex = VectorStoreIndex\n",
    "\n",
    "\n",
    "def index_documents(folder):\n",
    "    \"\"\" index your documents so that ChatGPT can use it to answer your questions.\"\"\"\n",
    "    \n",
    "    max_input_size = 4096\n",
    "    num_outputs = 512\n",
    "    max_chunk_overlap = 20\n",
    "    chunk_size_limit = 600\n",
    "\n",
    "    prompt_helper = PromptHelper(max_input_size,\n",
    "                                 num_outputs,\n",
    "                                 max_chunk_overlap,\n",
    "                                 chunk_size_limit=chunk_size_limit)\n",
    "\n",
    "    llm_predictor = LLMPredictor(\n",
    "        llm=ChatOpenAI(temperature=0.7,\n",
    "                       model_name=\"gpt-3.5-turbo\",\n",
    "                       max_tokens=num_outputs)\n",
    "    )\n",
    "\n",
    "    documents = SimpleDirectoryReader(folder).load_data()\n",
    "\n",
    "    index = GPTVectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        llm_predictor=llm_predictor,\n",
    "        prompt_helper=prompt_helper)\n",
    "\n",
    "    index.storage_context.persist(persist_dir=\"resources/\")\n",
    "    \n",
    "index_documents('resources\\\\ingest\\\\')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* The `PromptHelper` class helps us fill in the prompt, split the text,and fill in context information according to necessary token limitations.\n",
    "\n",
    "* You create an instance of the `LLMPredictor` class, which is a wrapper around an **LLMChain** from `Langchain`. You will make use of the LLM from OpenAI’s “gpt-3.5-turbo” model.\n",
    "\n",
    "* The `SimpleDirectoryReader` class loads the documents from the documents folder, which you will use to perform the indexing.\n",
    "\n",
    "* The GPTVectorStoreIndex’s `from_documents()` function performs the indexing using the documents that you have placed in the training documents folder.\n",
    "\n",
    "* Once the indexing is done, you persist it to storage using the storage_context.persist() function. \n",
    "\n",
    "* Persisting the index allows you to query the LLM at a later time without spending time performing the indexing again. By default, the index is saved to a file named vector_store.json."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "def my_chatGPT_bot(input_text):\n",
    "    \n",
    "    # load the index from vector_store.json\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"resources/\")\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "    # create a query engine to ask question\n",
    "    query_engine = index.as_query_engine()\n",
    "    response = query_engine.query(input_text)\n",
    "    return response.response\n",
    "\n",
    "my_chatGPT_bot('What is the population actually in Singapore?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
